{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, predictions_df, task_id, prompt):\n",
    "        \"\"\"\n",
    "        Initializes the Evaluation object.\n",
    "        \"\"\"\n",
    "        self.predictions = predictions_df\n",
    "        self.task_id = task_id\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def extract_numeric_answer(self, text):\n",
    "        \"\"\"\n",
    "        Extracts a numeric answer from the given text using a regular expression.\n",
    "        \"\"\"\n",
    "        if self.prompt == \"bnap\":\n",
    "            match = re.search(r'উত্তর:\\s.*?([+-]?\\d*\\.?\\d+)', text, re.DOTALL)\n",
    "        else:\n",
    "            match = re.search(r\"(?:Answer:|answer is:)\\s.*?([+-]?\\d*\\.?\\d+)\", str(text), re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return \"00000\"\n",
    "        \n",
    "    def extract_option_answer(self, text):\n",
    "        \"\"\"\n",
    "        Extracts an option answer from the input text.\n",
    "        For the 'bnap' prompt, it maps \"উত্তর ১\" to \"Option 1\" and \"উত্তর ২\" to \"Option 2\".\n",
    "        \"\"\"\n",
    "        if self.prompt == 'bnap':\n",
    "            match = re.search(r\"উত্তর:\\s*(.*)\", text, re.DOTALL)\n",
    "            if match:\n",
    "                answer = match.group(1).strip()\n",
    "                if \"উত্তর ১\" in answer:\n",
    "                    return \"Option 1\"\n",
    "                elif \"উত্তর ২\" in answer:\n",
    "                    return \"Option 2\"\n",
    "                else:\n",
    "                    return \"00000\"\n",
    "            else:\n",
    "                return \"00000\"\n",
    "        else:\n",
    "            match = re.search(r\"(?:Answer:|answer is:)\\s*(.*)\", text, re.DOTALL)\n",
    "            if match:\n",
    "                return match.group(1).strip()\n",
    "            else:\n",
    "                return \"00000\"\n",
    "\n",
    "    def extract_relation_answer(self, text):\n",
    "        \"\"\"\n",
    "        Extracts a relation answer from the input text and maps it to one of:\n",
    "        \"neutral\", \"contradiction\", or \"Entailment\".\n",
    "        \"\"\"\n",
    "        if self.prompt == 'bnap':\n",
    "            match = re.search(r\"উত্তর:\\s*(.*)\", text, re.DOTALL)\n",
    "            if match:\n",
    "                answer = match.group(1)\n",
    "                if \"নিরপেক্ষ\" in answer:\n",
    "                    return \"neutral\"\n",
    "                elif \"বিরোধ\" in answer:\n",
    "                    return \"contradiction\"\n",
    "                elif \"সমর্থন\" in answer:\n",
    "                    return \"Entailment\"\n",
    "                else:\n",
    "                    return \"00000\"\n",
    "            else:\n",
    "                return \"00000\"\n",
    "        else:\n",
    "            match = re.search(r\"(?:Answer:|answer is:)\\s*(.*)\", text, re.DOTALL)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "            else:\n",
    "                return \"00000\"\n",
    "\n",
    "    def is_direct_answer_match(self, correct_ans, model_output):\n",
    "        \"\"\"\n",
    "        Checks if the correct answer and model output match.\n",
    "        For 'bnap' prompt, the match is direct (case-sensitive), \n",
    "        otherwise it performs a case-insensitive substring check.\n",
    "        \"\"\"\n",
    "        if self.prompt == \"bnap\":\n",
    "            return correct_ans == model_output\n",
    "        else:\n",
    "            return correct_ans.lower() in model_output.lower()\n",
    "\n",
    "    def convert_bengali_digits_to_english(self, text):\n",
    "        \"\"\"\n",
    "        Converts Bengali numeric characters in the input text to their English equivalents.\n",
    "        \n",
    "        Returns:\n",
    "            str: The converted numeric text in English, or \"00000\" if no valid numeric characters are found.\n",
    "        \"\"\"\n",
    "        mapping = {\n",
    "            '০': '0',\n",
    "            '১': '1',\n",
    "            '২': '2',\n",
    "            '৩': '3',\n",
    "            '৪': '4',\n",
    "            '৫': '5',\n",
    "            '৬': '6',\n",
    "            '৭': '7',\n",
    "            '৮': '8',\n",
    "            '৯': '9',\n",
    "            '.': '.',\n",
    "            \"/\": \"/\"\n",
    "        }\n",
    "        # Retain only characters that are in our mapping or are valid digits/punctuation\n",
    "        english_text = ''.join(mapping.get(char, char) for char in text if char in '০১২৩৪৫৬৭৮৯10123456789./-')\n",
    "        return english_text if english_text else \"00000\"\n",
    "\n",
    "    def is_numeric_answer_match(self, ground_truth, model_ans):\n",
    "        \"\"\"\n",
    "        Compares a numeric ground truth answer with the model answer after converting Bengali digits.\n",
    "        \"\"\"\n",
    "        ground_truth_converted = self.convert_bengali_digits_to_english(ground_truth)\n",
    "        model_ans_converted = self.convert_bengali_digits_to_english(model_ans)\n",
    "        ground_truth_val = int(eval(ground_truth_converted) * 100) / 100\n",
    "        model_ans_val = int(eval(model_ans_converted) * 100) / 100\n",
    "        return ground_truth_val == model_ans_val\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model's accuracy on the predictions DataFrame by comparing the 'Answer' and \n",
    "        'Model Response' columns.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple containing the accuracy percentage and the number of exact matches.\n",
    "        \"\"\"\n",
    "        total_data = len(self.predictions)\n",
    "        \n",
    "        if self.task_id == \"task3\":\n",
    "            exact_matches = self.predictions.apply(\n",
    "                lambda row: self.is_direct_answer_match(\n",
    "                    row['Answer'], \n",
    "                    self.extract_option_answer(row['Model Response'])\n",
    "                ),\n",
    "                axis=1\n",
    "            ).sum()\n",
    "            \n",
    "        elif self.task_id == \"task5\":\n",
    "            exact_matches = self.predictions.apply(\n",
    "                lambda row: self.is_direct_answer_match(\n",
    "                    row['Answer'], \n",
    "                    self.extract_relation_answer(row['Model Response'])\n",
    "                ),\n",
    "                axis=1\n",
    "            ).sum()\n",
    "            \n",
    "        else:\n",
    "            exact_matches = self.predictions.apply(\n",
    "                lambda row: self.is_numeric_answer_match(\n",
    "                    row['Answer'], \n",
    "                    self.extract_numeric_answer(row['Model Response'])\n",
    "                ),\n",
    "                axis=1\n",
    "            ).sum()\n",
    "            \n",
    "        accuracy = (exact_matches / total_data) * 100\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the sample sizes for each task\n",
    "task_sizes = {\n",
    "    \"task1\": 150,\n",
    "    \"task2\": 250,\n",
    "    \"task3\": 150,\n",
    "    \"task4\": 150,\n",
    "    \"task5\": 150,\n",
    "    \"task6\": 150,\n",
    "}\n",
    "\n",
    "# Define the prompts and models\n",
    "prompts = [\"bnap\", \"xlp\", \"xcot\"]\n",
    "\n",
    "models = [\n",
    "    \"Mathstral_7B\", \n",
    "    \"Llama_3.3_70B\", \n",
    "    \"DeepSeek_R1_Distill_Llama_70B\",\n",
    "    \"Gpt_4o\", \n",
    "    \"Gemini_2.0_flash\", \n",
    "]\n",
    "\n",
    "# Tasks to process\n",
    "tasks = [1,2,3,4,5,6]\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {prompt: [] for prompt in prompts}\n",
    "\n",
    "# Loop through prompts, tasks, and models to collect results\n",
    "for prompt in prompts:\n",
    "    for model in models:\n",
    "        model_results = {\"Model\": model} \n",
    "        for task in tasks:\n",
    "            taskId = f\"task{task}\"\n",
    "            sample_size = task_sizes.get(taskId)\n",
    "            filename = f\"{model}_{prompt}_{taskId}_random_{sample_size}_responses.csv\"\n",
    "            csv_path = os.path.join(\"Codes\", \"Run_2\", \"Model Responses\", model, filename)\n",
    "                \n",
    "            \n",
    "            # Read the predictions DataFrame\n",
    "            predictions_df = pd.read_csv(csv_path)\n",
    "            \n",
    "            # Evaluate the predictions\n",
    "            evaluator = Evaluation(predictions_df, taskId, prompt)\n",
    "            accuracy = evaluator.evaluate_accuracy()\n",
    "            \n",
    "            # Store results\n",
    "            model_results[taskId] = accuracy\n",
    "        \n",
    "        # Calculate average score across tasks\n",
    "        model_results[\"Avg.\"] = sum(model_results[taskId] for taskId in model_results if taskId != \"Model\") / len(tasks)\n",
    "        \n",
    "        # Append results to corresponding prompt category\n",
    "        results[prompt].append(model_results)\n",
    "\n",
    "# Convert results to DataFrames\n",
    "df_bnap = pd.DataFrame(results[\"bnap\"])\n",
    "df_xlp = pd.DataFrame(results[\"xlp\"])\n",
    "df_xcot = pd.DataFrame(results[\"xcot\"])\n",
    "\n",
    "# Display results in tabular format\n",
    "print(\"--------------------------------------------BNaP Results--------------------------------------------\")\n",
    "print(df_bnap.to_string(index=False))\n",
    "print(\"--------------------------------------------XLP Results---------------------------------------------\")\n",
    "print(df_xlp.to_string(index=False))\n",
    "print(\"--------------------------------------------XCoT Results--------------------------------------------\")\n",
    "print(df_xcot.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4922963,
     "sourceId": 9292863,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5629927,
     "sourceId": 9529237,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5630033,
     "sourceId": 9529241,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5629921,
     "sourceId": 9529244,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5788741,
     "sourceId": 9530534,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5788725,
     "sourceId": 9530539,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5788719,
     "sourceId": 9530599,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5811886,
     "sourceId": 9540778,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5814541,
     "sourceId": 9544348,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6631093,
     "sourceId": 10700486,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

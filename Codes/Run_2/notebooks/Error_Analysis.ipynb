{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, predictions_df, task_id, prompt):\n",
    "        \"\"\"\n",
    "        Initializes the Evaluation object.\n",
    "        \"\"\"\n",
    "        self.predictions = predictions_df\n",
    "        self.task_id = task_id\n",
    "        self.prompt = prompt\n",
    "        \n",
    "        # Check the column names and set the response column name\n",
    "        self.response_column = self._get_response_column()\n",
    "        \n",
    "    def _get_response_column(self):\n",
    "        \"\"\"\n",
    "        Determines the correct column name for model responses.\n",
    "        \"\"\"\n",
    "        columns = self.predictions.columns\n",
    "        possible_names = ['Model Response', 'Model_Response', 'model_response', \n",
    "                         'Response', 'response', 'Output', 'output', 'Prediction']\n",
    "        \n",
    "        for name in possible_names:\n",
    "            if name in columns:\n",
    "                return name\n",
    "        \n",
    "        # If none of the expected names are found, use the second column \n",
    "        # (assuming first is 'Answer' and second is the response)\n",
    "        if len(columns) >= 2:\n",
    "            print(f\"Response column not found. Using column: {columns[1]}\")\n",
    "            return columns[1]\n",
    "        \n",
    "        raise ValueError(\"Could not determine model response column. Available columns: \" + str(columns.tolist()))\n",
    "\n",
    "    def extract_numeric_answer(self, text):\n",
    "        \"\"\"\n",
    "        Extracts a numeric answer from the given text using a regular expression.\n",
    "        \"\"\"\n",
    "        if self.prompt == \"bnap\":\n",
    "            match = re.search(r'উত্তর:\\s.*?([+-]?\\d*\\.?\\d+)', str(text), re.DOTALL)\n",
    "        else:\n",
    "            match = re.search(r\"(?:Answer:|answer is:)\\s.*?([+-]?\\d*\\.?\\d+)\", str(text), re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return \"00000\"\n",
    "        \n",
    "    def extract_option_answer(self, text):\n",
    "        \"\"\"\n",
    "        Extracts an option answer from the input text.\n",
    "        For the 'bnap' prompt, it maps \"উত্তর ১\" to \"Option 1\" and \"উত্তর ২\" to \"Option 2\".\n",
    "        \"\"\"\n",
    "        if self.prompt == \"bnap\":\n",
    "            match = re.search(r\"উত্তর:\\s*(.*)\", text, re.DOTALL)\n",
    "            if match:\n",
    "                answer = match.group(1).strip()\n",
    "                if \"উত্তর ১\" in answer:\n",
    "                    return \"Option 1\"\n",
    "                elif \"উত্তর ২\" in answer:\n",
    "                    return \"Option 2\"\n",
    "                else:\n",
    "                    return \"00000\"\n",
    "            else:\n",
    "                return \"00000\"\n",
    "        else:\n",
    "            match = re.search(r\"(?:Answer:|answer is:)\\s*(.*)\", text, re.DOTALL)\n",
    "            if match:\n",
    "                return match.group(1).strip()\n",
    "            else:\n",
    "                return \"00000\"\n",
    "\n",
    "    def extract_relation_answer(self, text):\n",
    "        \"\"\"\n",
    "        Extracts a relation answer from the input text and maps it to one of:\n",
    "        \"neutral\", \"contradiction\", or \"Entailment\".\n",
    "        \"\"\"\n",
    "        if self.prompt == \"bnap\":\n",
    "            match = re.search(r\"উত্তর:\\s*(.*)\", text, re.DOTALL)\n",
    "            if match:\n",
    "                answer = match.group(1)\n",
    "                if \"নিরপেক্ষ\" in answer:\n",
    "                    return \"neutral\"\n",
    "                elif \"বিরোধ\" in answer:\n",
    "                    return \"contradiction\"\n",
    "                elif \"সমর্থন\" in answer:\n",
    "                    return \"Entailment\"\n",
    "                else:\n",
    "                    return \"00000\"\n",
    "            else:\n",
    "                return \"00000\"\n",
    "        else:\n",
    "            match = re.search(r\"(?:Answer:|answer is:)\\s*(.*)\", str(text), re.DOTALL)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "            else:\n",
    "                return \"00000\"\n",
    "\n",
    "    def is_direct_answer_match(self, correct_ans, model_output):\n",
    "        \"\"\"\n",
    "        Checks if the correct answer and model output match.\n",
    "        For 'bnap' prompt, the match is direct (case-sensitive), \n",
    "        otherwise it performs a case-insensitive substring check.\n",
    "        \"\"\"\n",
    "        if self.prompt == \"bnap\":\n",
    "            return correct_ans == model_output\n",
    "        else:\n",
    "            return correct_ans.lower() in model_output.lower()\n",
    "\n",
    "    def convert_bengali_digits_to_english(self, text):\n",
    "        \"\"\"\n",
    "        Converts Bengali numeric characters in the input text to their English equivalents.\n",
    "        \n",
    "        Returns:\n",
    "            str: The converted numeric text in English, or \"00000\" if no valid numeric characters are found.\n",
    "        \"\"\"\n",
    "        mapping = {\n",
    "            '০': '0',\n",
    "            '১': '1',\n",
    "            '২': '2',\n",
    "            '৩': '3',\n",
    "            '৪': '4',\n",
    "            '৫': '5',\n",
    "            '৬': '6',\n",
    "            '৭': '7',\n",
    "            '৮': '8',\n",
    "            '৯': '9',\n",
    "            '.': '.',\n",
    "            \"/\": \"/\"\n",
    "        }\n",
    "        # Retain only characters that are in our mapping or are valid digits/punctuation\n",
    "        english_text = ''.join(mapping.get(char, char) for char in text if char in '০১২৩৪৫৬৭৮৯10123456789./-')\n",
    "        return english_text if english_text else \"00000\"\n",
    "\n",
    "    def is_numeric_answer_match(self, ground_truth, model_ans):\n",
    "        \"\"\"\n",
    "        Compares a numeric ground truth answer with the model answer after converting Bengali digits.\n",
    "        \"\"\"\n",
    "        ground_truth_converted = self.convert_bengali_digits_to_english(ground_truth)\n",
    "        model_ans_converted = self.convert_bengali_digits_to_english(model_ans)\n",
    "        ground_truth_val = int(eval(ground_truth_converted) * 100) / 100\n",
    "        model_ans_val = int(eval(model_ans_converted) * 100) / 100\n",
    "        return ground_truth_val == model_ans_val\n",
    "\n",
    "    def find_wrong_output_format(self, answer):\n",
    "        if answer == \"00000\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def evaluate_errors(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model's accuracy on the predictions DataFrame by comparing the 'Answer' and \n",
    "        model response columns.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple containing error percentages.\n",
    "        \"\"\"\n",
    "        total_data = len(self.predictions)\n",
    "        \n",
    "        # Use the correct response column name\n",
    "        response_column = self.response_column\n",
    "        print(f\"Using response column: {response_column}\")\n",
    "        \n",
    "        if self.task_id == \"task3\":\n",
    "            exact_matches = self.predictions.apply(\n",
    "                lambda row: self.is_direct_answer_match(\n",
    "                    row['Answer'], \n",
    "                    self.extract_option_answer(row[response_column])\n",
    "                ),\n",
    "                axis=1\n",
    "            ).sum()\n",
    "\n",
    "            wrong_format = self.predictions.apply(\n",
    "                lambda row: self.find_wrong_output_format(\n",
    "                    self.extract_option_answer(row[response_column])\n",
    "                ),\n",
    "                axis=1\n",
    "            ).sum()\n",
    "            \n",
    "        elif self.task_id == \"task5\":\n",
    "            exact_matches = self.predictions.apply(\n",
    "                lambda row: self.is_direct_answer_match(\n",
    "                    row['Answer'], \n",
    "                    self.extract_relation_answer(row[response_column])\n",
    "                ),\n",
    "                axis=1\n",
    "            ).sum()\n",
    "            \n",
    "            wrong_format = self.predictions.apply(\n",
    "                lambda row: self.find_wrong_output_format(\n",
    "                    self.extract_relation_answer(row[response_column])\n",
    "                ),\n",
    "                axis=1\n",
    "            ).sum()\n",
    "            \n",
    "        else:\n",
    "            exact_matches = self.predictions.apply(\n",
    "                lambda row: self.is_numeric_answer_match(\n",
    "                    row['Answer'], \n",
    "                    self.extract_numeric_answer(row[response_column])\n",
    "                ),\n",
    "                axis=1\n",
    "            ).sum()\n",
    "            \n",
    "            wrong_format = self.predictions.apply(\n",
    "                lambda row: self.find_wrong_output_format(\n",
    "                    self.extract_numeric_answer(row[response_column])\n",
    "                ),\n",
    "                axis=1\n",
    "            ).sum()\n",
    "            \n",
    "        wrong_predictions = (total_data - exact_matches)\n",
    "        wrong_prediction_percentage = (wrong_predictions / total_data) * 100\n",
    "        wrong_format_percentage = (wrong_format / total_data) * 100\n",
    "        correct_format = total_data - wrong_format\n",
    "        # wrong_calculation = correct_format - exact_matches\n",
    "        wrong_calculation = (wrong_predictions - wrong_format)\n",
    "        wrong_calculation_percentage = (wrong_calculation / correct_format) * 100\n",
    "\n",
    "        return wrong_prediction_percentage, wrong_format_percentage, wrong_calculation_percentage\n",
    "\n",
    "# Main code for processing and evaluation\n",
    "def main():\n",
    "    # Define the sample sizes for each task\n",
    "    task_sizes = {\n",
    "        \"task1\": 150,\n",
    "        \"task2\": 250,\n",
    "        \"task3\": 150,\n",
    "        \"task4\": 150,\n",
    "        \"task5\": 150,\n",
    "        \"task6\": 150,\n",
    "    }\n",
    "\n",
    "    # Define the prompts and models\n",
    "    prompts = [\"bnap\", \"xlp\", \"xcot\"]\n",
    "\n",
    "    models = [\n",
    "        \"Mathstral_7B\", \n",
    "        \"Llama_3.3_70B\", \n",
    "        \"DeepSeek_R1_Distill_Llama_70B\",\n",
    "        \"Gpt_4o\", \n",
    "        \"Gemini_2.0_flash\", \n",
    "    ]\n",
    "\n",
    "    # Tasks to process\n",
    "    tasks = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "    # Initialize results dictionary with the modified structure to store task-specific percentages\n",
    "    results_by_task = {\n",
    "        prompt: {\n",
    "            model: {\n",
    "                f\"task{task}\": {\"wrong_format_percentage\": 0, \"wrong_calculation_percentage\": 0}\n",
    "                for task in tasks\n",
    "            }\n",
    "            for model in models\n",
    "        }\n",
    "        for prompt in prompts\n",
    "    }\n",
    "\n",
    "    # Loop through prompts, tasks, and models to collect results\n",
    "    for prompt in prompts:\n",
    "        for model in models:\n",
    "            for task in tasks:\n",
    "                taskId = f\"task{task}\"\n",
    "                sample_size = task_sizes.get(taskId)\n",
    "                filename = f\"{model}_{prompt}_{taskId}_random_{sample_size}_responses.csv\"\n",
    "                csv_path = os.path.join(\"..\",\"Model Responses\", model, filename)\n",
    "                \n",
    "                try:\n",
    "                    # Read the predictions DataFrame\n",
    "                    print(f\"Processing {csv_path}\")\n",
    "                    predictions_df = pd.read_csv(csv_path)\n",
    "                    \n",
    "                    # Evaluate the predictions\n",
    "                    evaluator = Evaluation(predictions_df, taskId, prompt)\n",
    "                    wrong_prediction_percentage, wrong_format_percentage, wrong_calculation_percentage = evaluator.evaluate_errors()\n",
    "                    \n",
    "                    # Store the task-specific percentages\n",
    "                    results_by_task[prompt][model][taskId][\"wrong_format_percentage\"] = round(wrong_format_percentage, 2)\n",
    "                    results_by_task[prompt][model][taskId][\"wrong_calculation_percentage\"] = round(wrong_calculation_percentage, 2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_path}: {e}\")\n",
    "                    # Default values already set to 0 during initialization\n",
    "    \n",
    "    # Create tables for each prompt showing wrong format and wrong calculation percentages by task and model\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n\\n--------------------------------------------{prompt.upper()} Results (Run 2)--------------------------------------------\")\n",
    "        \n",
    "        # Create DataFrame for the current prompt\n",
    "        rows = []\n",
    "        for model in models:\n",
    "            # First row for wrong format percentages\n",
    "            wf_row = [model, \"WF%\"]\n",
    "            for task in tasks:\n",
    "                taskId = f\"task{task}\"\n",
    "                wf_row.append(results_by_task[prompt][model][taskId][\"wrong_format_percentage\"])\n",
    "            rows.append(wf_row)\n",
    "            \n",
    "            # Second row for wrong calculation percentages\n",
    "            wc_row = [\"\", \"WC%\"]\n",
    "            for task in tasks:\n",
    "                taskId = f\"task{task}\"\n",
    "                wc_row.append(results_by_task[prompt][model][taskId][\"wrong_calculation_percentage\"])\n",
    "            rows.append(wc_row)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        columns = [\"Model\", \"Metric\"] + [f\"Task {task}\" for task in tasks]\n",
    "        df_prompt = pd.DataFrame(rows, columns=columns)\n",
    "        \n",
    "        # Display the table\n",
    "        print(df_prompt.to_string(index=False))\n",
    "        \n",
    "        # Save to CSV\n",
    "        # df_prompt.to_csv(f\"{prompt}_error_percentages_by_task.csv\", index=False)\n",
    "    \n",
    "    # Optional: Create a consolidated CSV with all prompts\n",
    "    # This will create a multi-index DataFrame with prompt, model, and metric as indices\n",
    "    rows_all = []\n",
    "    for prompt in prompts:\n",
    "        for model in models:\n",
    "            for metric, label in [(\"wrong_format_percentage\", \"WF%\"), (\"wrong_calculation_percentage\", \"WC%\")]:\n",
    "                row = [prompt, model, label]\n",
    "                for task in tasks:\n",
    "                    taskId = f\"task{task}\"\n",
    "                    row.append(results_by_task[prompt][model][taskId][metric])\n",
    "                rows_all.append(row)\n",
    "    \n",
    "    columns_all = [\"Prompt\", \"Model\", \"Metric\"] + [f\"Task {task}\" for task in tasks]\n",
    "    # df_all = pd.DataFrame(rows_all, columns=columns_all)\n",
    "    # df_all.to_csv(\"all_error_percentages_by_task.csv\", index=False)\n",
    "    \n",
    "    # print(\"\\nDetailed error percentages by task saved to CSV files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4922963,
     "sourceId": 9292863,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5629927,
     "sourceId": 9529237,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5630033,
     "sourceId": 9529241,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5629921,
     "sourceId": 9529244,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5788741,
     "sourceId": 9530534,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5788725,
     "sourceId": 9530539,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5788719,
     "sourceId": 9530599,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5811886,
     "sourceId": 9540778,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5814541,
     "sourceId": 9544348,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6631093,
     "sourceId": 10700486,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

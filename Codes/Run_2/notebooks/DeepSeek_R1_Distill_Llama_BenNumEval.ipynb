{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_together in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.3.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain_together) (3.11.12)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain_together) (0.3.34)\n",
      "Requirement already satisfied: langchain-openai<0.4,>=0.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain_together) (0.3.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain_together) (2.31.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (6.0.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (1.18.3)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (0.3.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (6.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (2.10.6)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-openai<0.4,>=0.3->langchain_together) (1.61.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-openai<0.4,>=0.3->langchain_together) (0.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_together) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_together) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_together) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_together) (2023.7.22)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_together) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\acer\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_together) (2.27.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai<0.4,>=0.3->langchain_together) (2022.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\acer\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T06:35:37.187273Z",
     "iopub.status.busy": "2025-02-07T06:35:37.186863Z",
     "iopub.status.idle": "2025-02-07T06:35:37.194103Z",
     "shell.execute_reply": "2025-02-07T06:35:37.192965Z",
     "shell.execute_reply.started": "2025-02-07T06:35:37.187244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from langchain_together import ChatTogether\n",
    "api_key = \"api\"\n",
    "\n",
    "def load_model(prompting_type, model_name=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\", temperature=1, top_p=1.0, max_output_tokens=512):\n",
    "    model = ChatTogether(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_output_tokens,\n",
    "            top_p=top_p,\n",
    "            together_api_key=api_key\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T06:35:39.222599Z",
     "iopub.status.busy": "2025-02-07T06:35:39.22223Z",
     "iopub.status.idle": "2025-02-07T06:35:39.228776Z",
     "shell.execute_reply": "2025-02-07T06:35:39.227292Z",
     "shell.execute_reply.started": "2025-02-07T06:35:39.222571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(task):\n",
    "    \"\"\"\n",
    "    Load dataset for a given task.\n",
    "    \"\"\"\n",
    "    task1 = pd.read_csv(\"../BenNumEval_run_2_random_1000/Task1_random_150.csv\")\n",
    "    task2 = pd.read_csv(\"../BenNumEval_run_2_random_1000/Task2_random_250.csv\")\n",
    "    task3 = pd.read_csv(\"../BenNumEval_run_2_random_1000/Task3_random_150.csv\")\n",
    "    task4 = pd.read_csv(\"../BenNumEval_run_2_random_1000/Task4_random_150.csv\")\n",
    "    task5 = pd.read_csv(\"../BenNumEval_run_2_random_1000/Task5_random_150.csv\")\n",
    "    task6 = pd.read_csv(\"../BenNumEval_run_2_random_1000/Task6_random_150.csv\")\n",
    "\n",
    "    task_dict = {\n",
    "        \"task1\": task1,\n",
    "        \"task2\": task2,\n",
    "        \"task3\": task3,\n",
    "        \"task4\": task4,\n",
    "        \"task5\": task5,\n",
    "        \"task6\": task6\n",
    "    }\n",
    "    return task_dict.get(task, \"Invalid task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T06:35:40.749135Z",
     "iopub.status.busy": "2025-02-07T06:35:40.748712Z",
     "iopub.status.idle": "2025-02-07T06:35:40.759031Z",
     "shell.execute_reply": "2025-02-07T06:35:40.757537Z",
     "shell.execute_reply.started": "2025-02-07T06:35:40.749105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "xlp_prompt_template_task1246 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a mathematical reasoning problem provided in Bengali and deliver the solution in English.\\n\\n\"\n",
    "    \"Please adhere to the following format:\\n\"\n",
    "    \"1. Translate the Bengali problem into English for clarity.\\n\"\n",
    "    \"2. Conclude with the final numerical answer, formatted as **\\\"Answer: [num]\\\"**.\\n\\n\"\n",
    "    \"# Problem: {question}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "xcot_prompt_template_task1246 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a mathematical reasoning problem provided in Bengali and deliver the solution in English.\\n\\n\"\n",
    "    \"Please adhere to the following format:\\n\"\n",
    "    \"1. Translate the Bengali problem into English for clarity.\\n\"\n",
    "    \"2. Provide a detailed step-by-step solution with explanations in English, following the logical flow of reasoning.\\n\"\n",
    "    \"3. Conclude with the final numerical answer, formatted as **\\\"Answer: [num]\\\"**.\\n\\n\"\n",
    "    \"# Problem: {question}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "bnap_prompt_template_task1246 = (\n",
    "    \"# নির্দেশাবলী:\\n\"\n",
    "    \"আপনি একজন গণিত বিশেষজ্ঞ এআই মডেল, যিনি বাংলা ভাষায় সম্পূর্ণভাবে দক্ষ। আপনার কাজ হলো প্রদত্ত গাণিতিক সমস্যার বিশদভাবে সমাধান করা এবং উত্তরটি বাংলায় প্রদান করা।\\n\"\n",
    "    \"আপনাকে অবশ্যই চূড়ান্ত সাংখ্যিক উত্তরটি নিম্নলিখিত ফরম্যাটে উপস্থাপন করতে হবে: **\\\"উত্তর: [সংখ্যা]\\\"**।\\n\\n\"\n",
    "    \"# সমস্যা: {question}\\n\\n\"\n",
    "    \"# সমাধান:\"\n",
    ")\n",
    "\n",
    "xlp_prompt_template_task3 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a mathematical reasoning problem provided in Bengali and choose the correct option.\\n\\n\"\n",
    "    \"Please adhere to the following format:\\n\"\n",
    "    \"1. Translate the Bengali question and options into English for clarity.\\n\"\n",
    "    \"2. Conclude by selecting the correct option, formatted as **\\\"Answer: [Option]\\\"**. The possible options are **\\\"Option 1\\\"** or **\\\"Option 2\\\"**.\\n\\n\"\n",
    "    \"# Question: {question}\\n\"\n",
    "    \"# Option 1: {option1}\\n\"\n",
    "    \"# Option 2: {option2}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "xcot_prompt_template_task3 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a mathematical reasoning problem provided in Bengali and choose the correct option.\\n\\n\"\n",
    "    \"Please adhere to the following format:\\n\"\n",
    "    \"1. Translate the Bengali question and options into English for clarity.\\n\"\n",
    "    \"2. Provide a detailed step-by-step solution with explanations in English, following the logical flow of reasoning.\\n\"\n",
    "    \"3. Conclude by selecting the correct option, formatted as **\\\"Answer: [Option]\\\"**. The possible options are **\\\"Option 1\\\"** or **\\\"Option 2\\\"**.\\n\\n\"\n",
    "    \"# Question: {question}\\n\"\n",
    "    \"# Option 1: {option1}\\n\"\n",
    "    \"# Option 2: {option2}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "\n",
    "bnap_prompt_template_task3 = (\n",
    "    \"# নির্দেশাবলী:\\n\"\n",
    "    \"আপনি একজন দক্ষ গণিত বিশেষজ্ঞ এআই, যিনি বাংলা ভাষায় পারদর্শী। আপনার কাজ হলো প্রদত্ত গাণিতিক সমস্যার বিশ্লেষণ করে সঠিক উত্তর নির্বাচন করা।\\n\\n\"\n",
    "    \"আপনার উত্তর অবশ্যই *নির্দিষ্ট বিন্যাসে* প্রদান করতে হবে:\\n\"\n",
    "    \"**\\\"উত্তর: [সঠিক সম্ভাব্য উত্তর]\\\"**\\n\" \n",
    "    \"যেখানে **[সঠিক সম্ভাব্য উত্তর]** হবে **\\\"উত্তর ১\\\"** অথবা **\\\"উত্তর ২\\\"**।\\n\\n\"\n",
    "    \"---\\n\" \n",
    "    \"# সমস্যা: {question}\\n\"\n",
    "    \"# সম্ভাব্য উত্তরসমূহ:\\n\"\n",
    "    \"# সম্ভাব্য উত্তর ১: {option1}\\n\"\n",
    "    \"# সম্ভাব্য উত্তর ২: {option2}\\n\"  \n",
    "    \"---\\n\\n\" \n",
    "    \"# সমাধান:\"\n",
    "    )\n",
    "\n",
    "\n",
    "xlp_prompt_template_task5 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a Quantitative Natural Language Inference (QNLI) problem presented in Bengali. You need to determine the relationship between the premise and the hypothesis.\\n\\n\"\n",
    "    \"Please adhere to the following format:\\n\"\n",
    "    \"1. Translate both the premise and hypothesis from Bengali to English for clarity.\\n\"\n",
    "    \"2. Conclude by selecting the correct option, formatted as **\\\"Answer: [Option]\\\"**. The possible options are **\\\"Entailment\\\"**, **\\\"Neutral\\\"**, or **\\\"Contradiction\\\"**.\\n\\n\"\n",
    "    \"# Premise: {premise}\\n\"\n",
    "    \"# Hypothesis: {hypothesis}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "xcot_prompt_template_task5 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a Quantitative Natural Language Inference (QNLI) problem presented in Bengali. You need to determine the relationship between the premise and the hypothesis.\\n\\n\"\n",
    "    \"Please follow these steps:\\n\"\n",
    "    \"1. Translate both the premise and hypothesis from Bengali to English for clarity.\\n\"\n",
    "    \"2. Provide a step-by-step explanation of your reasoning process.\\n\"\n",
    "    \"3. Conclude by selecting the correct option, formatted as **\\\"Answer: [Option]\\\"**. The possible options are **\\\"Entailment\\\"**, **\\\"Neutral\\\"**, or **\\\"Contradiction\\\"**.\\n\\n\"\n",
    "    \"# Premise: {premise}\\n\"\n",
    "    \"# Hypothesis: {hypothesis}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "bnap_prompt_template_task5 = (\n",
    "    \"# নির্দেশাবলী:\\n\"\n",
    "    \"আপনি একজন গণিত বিশেষজ্ঞ AI মডেল, যিনি বাংলা ভাষায় দক্ষ। আপনার কাজ হলো একটি গাণিতিক ভাষাগত অনুমান সমস্যার সমাধান করা। আপনাকে প্রদত্ত পূর্বধারণা ও অনুমান এর মধ্যে সম্পর্ক নির্ধারণ করতে হবে।\\n\\n\"\n",
    "    \"অনুগ্রহ করে নিম্নলিখিত বিন্যাস অনুসরণ করুন:\\n\"\n",
    "    \"প্রদত্ত পূর্বধারণা ও অনুমান এর মধ্যে সম্পর্ক বিষয়ে চূড়ান্ত সিদ্ধান্ত নিন এবং সঠিক উত্তরটি **\\\"উত্তর: [সম্ভাব্য সঠিক উত্তর]\\\"** এই ভাবে প্রদান করুন। উত্তরের সম্ভাব্য বিকল্পগুলি হল: **\\\"সমর্থন\\\"**, **\\\"নিরপেক্ষ\\\"**, অথবা **\\\"বিরোধ\\\"**।\\n\\n\"\n",
    "    \"# পূর্বধারণা: {premise}\\n\"\n",
    "    \"# অনুমান: {hypothesis}\\n\\n\"\n",
    "    \"# সমাধান:\"\n",
    ")\n",
    "\n",
    "# Map prompt template names to their content\n",
    "prompt_map = {\n",
    "    \"xlp_prompt_template_task1246\": xlp_prompt_template_task1246,\n",
    "    \"xcot_prompt_template_task1246\": xcot_prompt_template_task1246,\n",
    "    \"bnap_prompt_template_task1246\": bnap_prompt_template_task1246,\n",
    "    \"xlp_prompt_template_task3\": xlp_prompt_template_task3,\n",
    "    \"xcot_prompt_template_task3\": xcot_prompt_template_task3,\n",
    "    \"bnap_prompt_template_task3\": bnap_prompt_template_task3,\n",
    "    \"xlp_prompt_template_task5\": xlp_prompt_template_task5,\n",
    "    \"xcot_prompt_template_task5\": xcot_prompt_template_task5,\n",
    "    \"bnap_prompt_template_task5\": bnap_prompt_template_task5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T06:48:37.695698Z",
     "iopub.status.busy": "2025-02-07T06:48:37.695204Z",
     "iopub.status.idle": "2025-02-07T06:48:37.70851Z",
     "shell.execute_reply": "2025-02-07T06:48:37.706737Z",
     "shell.execute_reply.started": "2025-02-07T06:48:37.695665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "prompts = [\"xlp\", \"xcot\", \"bnap\"]\n",
    "model_name = \"DeepSeek-R1-Distill-Llama-70B-free\"\n",
    "\n",
    "rate_limit = 6.0  # queries per minute\n",
    "sleep_interval = 60.0 / rate_limit  # 10 seconds per query\n",
    "\n",
    "def process_prompts_for_task(task_id, data):\n",
    "    \"\"\"\n",
    "    Process a given task with the provided dataset and for each prompt type.\n",
    "    \"\"\"\n",
    "    for prompt in prompts:\n",
    "        responses = []\n",
    "        chat_histories = []\n",
    "        num_instances = len(data)\n",
    "        print(f\"Processing Task {task_id} with prompt {prompt} for {num_instances} instances...\")\n",
    "\n",
    "        for i in range(num_instances):\n",
    "            start_time = time.time()\n",
    "            # Prepare variables for formatting prompt based on task type\n",
    "            if task_id in [1, 2, 4, 6]:\n",
    "                question = data.iloc[i]['Question']\n",
    "                formatted_prompt = prompt_map.get(f\"{prompt}_prompt_template_task1246\").format(question=question)\n",
    "            elif task_id == 3:\n",
    "                question = data.iloc[i]['Question']\n",
    "                option1 = data.iloc[i]['option1']\n",
    "                option2 = data.iloc[i]['option2']\n",
    "                formatted_prompt = prompt_map.get(f\"{prompt}_prompt_template_task{task_id}\").format(\n",
    "                    question=question, option1=option1, option2=option2\n",
    "                )\n",
    "            elif task_id == 5:\n",
    "                # For Task 5, data contains 'Premise' and 'Hypothesis'\n",
    "                premise = data.iloc[i]['Premise']\n",
    "                hypothesis = data.iloc[i]['Hypothesis']\n",
    "                formatted_prompt = prompt_map.get(f\"{prompt}_prompt_template_task{task_id}\").format(\n",
    "                    premise=premise, hypothesis=hypothesis\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Invalid task id: {task_id}\")\n",
    "                continue\n",
    "\n",
    "            # Start a new chat session and send the formatted prompt\n",
    "            model = load_model(prompting_type=prompt)\n",
    "            response = model.predict(formatted_prompt)\n",
    "            responses.append(response)\n",
    "\n",
    "            print(f\"\\033[91mTask {task_id} | Prompt {prompt} | Question No: {i}\\033[0m\")\n",
    "            \n",
    "            # Calculate elapsed time and sleep for the remaining time to complete 10 seconds.\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_sleep = sleep_interval - elapsed_time\n",
    "            if remaining_sleep > 0:\n",
    "                print(f\"Elapsed time: {elapsed_time:.2f}s | Remaining sleep: {remaining_sleep:.2f}s\")\n",
    "                time.sleep(remaining_sleep)\n",
    "                print(\"Sleeping for\", remaining_sleep, \"seconds...\")\n",
    "        \n",
    "        # Prepare the dataframe to save the responses and chat histories\n",
    "        data_with_solutions = copy.deepcopy(data)\n",
    "        data_with_solutions[\"Model Response\"] = responses\n",
    "        \n",
    "        # Save the responses to a CSV file\n",
    "        output_filename = f\"{model_name}_{prompt}_task{task_id}_random_{num_instances}_responses.csv\"\n",
    "        output_path = os.path.join(\"..\", \"Model Responses\", \"Llama_3.3_70B\", str(output_filename))\n",
    "        data_with_solutions.to_csv(output_path, index=False)\n",
    "        print(f\"Saved responses to {output_path}\")\n",
    "        \n",
    "        # Optionally, print a sample response (if exists)\n",
    "        print(data_with_solutions.iloc[10][\"Model Response\"])\n",
    "        \n",
    "        # print(\"Sleeping for 5 seconds before next prompt...\")\n",
    "        # time.sleep(5)\n",
    "    \n",
    "    print(\"Completed all prompts for Task\", task_id)\n",
    "    # print(\"Sleeping for 5 seconds before next task...\")\n",
    "    # time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T06:48:37.910111Z",
     "iopub.status.busy": "2025-02-07T06:48:37.909659Z",
     "iopub.status.idle": "2025-02-07T06:48:59.353903Z",
     "shell.execute_reply": "2025-02-07T06:48:59.352269Z",
     "shell.execute_reply.started": "2025-02-07T06:48:37.91008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing for Task 1\n",
      "Processing Task 1 with prompt xlp for 150 instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_26724\\2727529168.py:44: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = model.predict(formatted_prompt)\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'id': 'nvUmY89-2j9zxn-945693dbad23d058', 'error': {'message': 'Invalid API key provided. You can find your API key at https://api.together.xyz/settings/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tasks processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 21\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting processing for Task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     process_task(task)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tasks processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mprocess_task\u001b[1;34m(task_id)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# For tasks 1, 2, 4, 6, 3, and 5 the processing is handled inside process_prompts_for_task.\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m process_prompts_for_task(task_id, data)\n",
      "Cell \u001b[1;32mIn[9], line 44\u001b[0m, in \u001b[0;36mprocess_prompts_for_task\u001b[1;34m(task_id, data)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Start a new chat session and send the formatted prompt\u001b[39;00m\n\u001b[0;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(prompting_type\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[1;32m---> 44\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(formatted_prompt)\n\u001b[0;32m     45\u001b[0m responses\u001b[38;5;241m.\u001b[39mappend(response)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[91mTask \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Prompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Question No: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     emit_warning()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1128\u001b[0m, in \u001b[0;36mBaseChatModel.predict\u001b[1;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, stop: Optional[Sequence[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m   1126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m   1127\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[1;32m-> 1128\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m([HumanMessage(content\u001b[38;5;241m=\u001b[39mtext)], stop\u001b[38;5;241m=\u001b[39m_stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     emit_warning()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1091\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[1;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1090\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m-> 1091\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1092\u001b[0m         [messages], stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1093\u001b[0m     )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[0;32m   1095\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 690\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    691\u001b[0m                 m,\n\u001b[0;32m    692\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    693\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    694\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    695\u001b[0m             )\n\u001b[0;32m    696\u001b[0m         )\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    926\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    927\u001b[0m         )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:790\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 790\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:863\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    860\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    861\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    862\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    866\u001b[0m             {\n\u001b[0;32m    867\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    868\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    869\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    870\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    871\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    872\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    873\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    874\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    875\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    876\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    877\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    878\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    879\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    880\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    881\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    882\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    883\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    884\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    885\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    886\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    887\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    888\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    889\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    890\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    891\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    892\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    893\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    894\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    895\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    896\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    897\u001b[0m             },\n\u001b[0;32m    898\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    899\u001b[0m         ),\n\u001b[0;32m    900\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    901\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    902\u001b[0m         ),\n\u001b[0;32m    903\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    904\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    905\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    906\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1282\u001b[0m     )\n\u001b[1;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    961\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    962\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    963\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    964\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    965\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    966\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1073\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'id': 'nvUmY89-2j9zxn-945693dbad23d058', 'error': {'message': 'Invalid API key provided. You can find your API key at https://api.together.xyz/settings/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "def process_task(task_id):\n",
    "    \"\"\"\n",
    "    Load dataset for the task and process based on task type.\n",
    "    \"\"\"\n",
    "    dataset_key = f\"task{task_id}\"\n",
    "    data = load_dataset(dataset_key)\n",
    "    if isinstance(data, str):\n",
    "        print(f\"Error loading dataset for task: {task_id}\")\n",
    "        return\n",
    "\n",
    "    # For tasks 1, 2, 4, 6, 3, and 5 the processing is handled inside process_prompts_for_task.\n",
    "    process_prompts_for_task(task_id, data)\n",
    "\n",
    "def main():\n",
    "    for task in [1, 2, 3, 4, 5, 6]:\n",
    "        print(f\"Starting processing for Task {task}\")\n",
    "        process_task(task)\n",
    "    print(\"All tasks processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4922963,
     "sourceId": 9292863,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6604216,
     "sourceId": 10700644,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

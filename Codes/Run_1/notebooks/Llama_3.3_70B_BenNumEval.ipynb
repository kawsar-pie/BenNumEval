{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_together in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain_together) (3.11.12)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain_together) (0.3.34)\n",
      "Requirement already satisfied: langchain-openai<0.4,>=0.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain_together) (0.3.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain_together) (2.31.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (6.0.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_together) (1.18.3)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (0.3.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (6.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_together) (2.10.6)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-openai<0.4,>=0.3->langchain_together) (1.61.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-openai<0.4,>=0.3->langchain_together) (0.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_together) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_together) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_together) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_together) (2023.7.22)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_together) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\acer\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_together) (2.27.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai<0.4,>=0.3->langchain_together) (2022.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\acer\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_together) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain_together) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T06:35:37.187273Z",
     "iopub.status.busy": "2025-02-07T06:35:37.186863Z",
     "iopub.status.idle": "2025-02-07T06:35:37.194103Z",
     "shell.execute_reply": "2025-02-07T06:35:37.192965Z",
     "shell.execute_reply.started": "2025-02-07T06:35:37.187244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "# from together import Together\n",
    "from langchain_together import ChatTogether\n",
    "api_key = \"api\"\n",
    "\n",
    "def load_model(prompting_type, model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", temperature=1, top_p=1.0, max_output_tokens=512):\n",
    "    model = ChatTogether(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_output_tokens,\n",
    "            top_p=top_p,\n",
    "            together_api_key=api_key\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T06:35:39.222599Z",
     "iopub.status.busy": "2025-02-07T06:35:39.22223Z",
     "iopub.status.idle": "2025-02-07T06:35:39.228776Z",
     "shell.execute_reply": "2025-02-07T06:35:39.227292Z",
     "shell.execute_reply.started": "2025-02-07T06:35:39.222571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(task):\n",
    "    \"\"\"\n",
    "    Load dataset for a given task.\n",
    "    \"\"\"\n",
    "    task1 = pd.read_csv(\"../BenNumEval_run_1_random_1000/Task1_random_150.csv\")\n",
    "    task2 = pd.read_csv(\"../BenNumEval_run_1_random_1000/Task2_random_250.csv\")\n",
    "    task3 = pd.read_csv(\"../BenNumEval_run_1_random_1000/Task3_random_150.csv\")\n",
    "    task4 = pd.read_csv(\"../BenNumEval_run_1_random_1000/Task4_random_150.csv\")\n",
    "    task5 = pd.read_csv(\"../BenNumEval_run_1_random_1000/Task5_random_150.csv\")\n",
    "    task6 = pd.read_csv(\"../BenNumEval_run_1_random_1000/Task6_random_150.csv\")\n",
    "\n",
    "    task_dict = {\n",
    "        \"task1\": task1,\n",
    "        \"task2\": task2,\n",
    "        \"task3\": task3,\n",
    "        \"task4\": task4,\n",
    "        \"task5\": task5,\n",
    "        \"task6\": task6\n",
    "    }\n",
    "    return task_dict.get(task, \"Invalid task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T06:35:40.749135Z",
     "iopub.status.busy": "2025-02-07T06:35:40.748712Z",
     "iopub.status.idle": "2025-02-07T06:35:40.759031Z",
     "shell.execute_reply": "2025-02-07T06:35:40.757537Z",
     "shell.execute_reply.started": "2025-02-07T06:35:40.749105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "xlp_prompt_template_task1246 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a mathematical reasoning problem provided in Bengali and deliver the solution in English.\\n\\n\"\n",
    "    \"Please adhere to the following format:\\n\"\n",
    "    \"1. Translate the Bengali problem into English for clarity.\\n\"\n",
    "    \"2. Conclude with the final numerical answer, formatted as **\\\"Answer: [num]\\\"**.\\n\\n\"\n",
    "    \"# Problem: {question}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "xcot_prompt_template_task1246 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a mathematical reasoning problem provided in Bengali and deliver the solution in English.\\n\\n\"\n",
    "    \"Please adhere to the following format:\\n\"\n",
    "    \"1. Translate the Bengali problem into English for clarity.\\n\"\n",
    "    \"2. Provide a detailed step-by-step solution with explanations in English, following the logical flow of reasoning.\\n\"\n",
    "    \"3. Conclude with the final numerical answer, formatted as **\\\"Answer: [num]\\\"**.\\n\\n\"\n",
    "    \"# Problem: {question}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "bnap_prompt_template_task1246 = (\n",
    "    \"# নির্দেশাবলী:\\n\"\n",
    "    \"আপনি একজন গণিত বিশেষজ্ঞ এআই মডেল, যিনি বাংলা ভাষায় সম্পূর্ণভাবে দক্ষ। আপনার কাজ হলো প্রদত্ত গাণিতিক সমস্যার বিশদভাবে সমাধান করা এবং উত্তরটি বাংলায় প্রদান করা।\\n\"\n",
    "    \"আপনাকে অবশ্যই চূড়ান্ত সাংখ্যিক উত্তরটি নিম্নলিখিত ফরম্যাটে উপস্থাপন করতে হবে: **\\\"উত্তর: [সংখ্যা]\\\"**।\\n\\n\"\n",
    "    \"# সমস্যা: {question}\\n\\n\"\n",
    "    \"# সমাধান:\"\n",
    ")\n",
    "\n",
    "xlp_prompt_template_task3 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a mathematical reasoning problem provided in Bengali and choose the correct option.\\n\\n\"\n",
    "    \"Please adhere to the following format:\\n\"\n",
    "    \"1. Translate the Bengali question and options into English for clarity.\\n\"\n",
    "    \"2. Conclude by selecting the correct option, formatted as **\\\"Answer: [Option]\\\"**. The possible options are **\\\"Option 1\\\"** or **\\\"Option 2\\\"**.\\n\\n\"\n",
    "    \"# Question: {question}\\n\"\n",
    "    \"# Option 1: {option1}\\n\"\n",
    "    \"# Option 2: {option2}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "xcot_prompt_template_task3 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a mathematical reasoning problem provided in Bengali and choose the correct option.\\n\\n\"\n",
    "    \"Please adhere to the following format:\\n\"\n",
    "    \"1. Translate the Bengali question and options into English for clarity.\\n\"\n",
    "    \"2. Provide a detailed step-by-step solution with explanations in English, following the logical flow of reasoning.\\n\"\n",
    "    \"3. Conclude by selecting the correct option, formatted as **\\\"Answer: [Option]\\\"**. The possible options are **\\\"Option 1\\\"** or **\\\"Option 2\\\"**.\\n\\n\"\n",
    "    \"# Question: {question}\\n\"\n",
    "    \"# Option 1: {option1}\\n\"\n",
    "    \"# Option 2: {option2}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "\n",
    "bnap_prompt_template_task3 = (\n",
    "    \"# নির্দেশাবলী:\\n\"\n",
    "    \"আপনি একজন দক্ষ গণিত বিশেষজ্ঞ এআই, যিনি বাংলা ভাষায় পারদর্শী। আপনার কাজ হলো প্রদত্ত গাণিতিক সমস্যার বিশ্লেষণ করে সঠিক উত্তর নির্বাচন করা।\\n\\n\"\n",
    "    \"আপনার উত্তর অবশ্যই *নির্দিষ্ট বিন্যাসে* প্রদান করতে হবে:\\n\"\n",
    "    \"**\\\"উত্তর: [সঠিক সম্ভাব্য উত্তর]\\\"**\\n\" \n",
    "    \"যেখানে **[সঠিক সম্ভাব্য উত্তর]** হবে **\\\"উত্তর ১\\\"** অথবা **\\\"উত্তর ২\\\"**।\\n\\n\"\n",
    "    \"---\\n\" \n",
    "    \"# সমস্যা: {question}\\n\"\n",
    "    \"# সম্ভাব্য উত্তরসমূহ:\\n\"\n",
    "    \"# সম্ভাব্য উত্তর ১: {option1}\\n\"\n",
    "    \"# সম্ভাব্য উত্তর ২: {option2}\\n\"  \n",
    "    \"---\\n\\n\" \n",
    "    \"# সমাধান:\"\n",
    "    )\n",
    "\n",
    "\n",
    "xlp_prompt_template_task5 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a Quantitative Natural Language Inference (QNLI) problem presented in Bengali. You need to determine the relationship between the premise and the hypothesis.\\n\\n\"\n",
    "    \"Please adhere to the following format:\\n\"\n",
    "    \"1. Translate both the premise and hypothesis from Bengali to English for clarity.\\n\"\n",
    "    \"2. Conclude by selecting the correct option, formatted as **\\\"Answer: [Option]\\\"**. The possible options are **\\\"Entailment\\\"**, **\\\"Neutral\\\"**, or **\\\"Contradiction\\\"**.\\n\\n\"\n",
    "    \"# Premise: {premise}\\n\"\n",
    "    \"# Hypothesis: {hypothesis}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "xcot_prompt_template_task5 = (\n",
    "    \"# Instructions:\\n\"\n",
    "    \"You are a Math Expert AI model proficient in both Bengali and English. Your task is to solve a Quantitative Natural Language Inference (QNLI) problem presented in Bengali. You need to determine the relationship between the premise and the hypothesis.\\n\\n\"\n",
    "    \"Please follow these steps:\\n\"\n",
    "    \"1. Translate both the premise and hypothesis from Bengali to English for clarity.\\n\"\n",
    "    \"2. Provide a step-by-step explanation of your reasoning process.\\n\"\n",
    "    \"3. Conclude by selecting the correct option, formatted as **\\\"Answer: [Option]\\\"**. The possible options are **\\\"Entailment\\\"**, **\\\"Neutral\\\"**, or **\\\"Contradiction\\\"**.\\n\\n\"\n",
    "    \"# Premise: {premise}\\n\"\n",
    "    \"# Hypothesis: {hypothesis}\\n\\n\"\n",
    "    \"# Response:\"\n",
    ")\n",
    "\n",
    "bnap_prompt_template_task5 = (\n",
    "    \"# নির্দেশাবলী:\\n\"\n",
    "    \"আপনি একজন গণিত বিশেষজ্ঞ AI মডেল, যিনি বাংলা ভাষায় দক্ষ। আপনার কাজ হলো একটি গাণিতিক ভাষাগত অনুমান সমস্যার সমাধান করা। আপনাকে প্রদত্ত পূর্বধারণা ও অনুমান এর মধ্যে সম্পর্ক নির্ধারণ করতে হবে।\\n\\n\"\n",
    "    \"অনুগ্রহ করে নিম্নলিখিত বিন্যাস অনুসরণ করুন:\\n\"\n",
    "    \"প্রদত্ত পূর্বধারণা ও অনুমান এর মধ্যে সম্পর্ক বিষয়ে চূড়ান্ত সিদ্ধান্ত নিন এবং সঠিক উত্তরটি **\\\"উত্তর: [সম্ভাব্য সঠিক উত্তর]\\\"** এই ভাবে প্রদান করুন। উত্তরের সম্ভাব্য বিকল্পগুলি হল: **\\\"সমর্থন\\\"**, **\\\"নিরপেক্ষ\\\"**, অথবা **\\\"বিরোধ\\\"**।\\n\\n\"\n",
    "    \"# পূর্বধারণা: {premise}\\n\"\n",
    "    \"# অনুমান: {hypothesis}\\n\\n\"\n",
    "    \"# সমাধান:\"\n",
    ")\n",
    "\n",
    "# Map prompt template names to their content\n",
    "prompt_map = {\n",
    "    \"xlp_prompt_template_task1246\": xlp_prompt_template_task1246,\n",
    "    \"xcot_prompt_template_task1246\": xcot_prompt_template_task1246,\n",
    "    \"bnap_prompt_template_task1246\": bnap_prompt_template_task1246,\n",
    "    \"xlp_prompt_template_task3\": xlp_prompt_template_task3,\n",
    "    \"xcot_prompt_template_task3\": xcot_prompt_template_task3,\n",
    "    \"bnap_prompt_template_task3\": bnap_prompt_template_task3,\n",
    "    \"xlp_prompt_template_task5\": xlp_prompt_template_task5,\n",
    "    \"xcot_prompt_template_task5\": xcot_prompt_template_task5,\n",
    "    \"bnap_prompt_template_task5\": bnap_prompt_template_task5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T06:48:37.695698Z",
     "iopub.status.busy": "2025-02-07T06:48:37.695204Z",
     "iopub.status.idle": "2025-02-07T06:48:37.70851Z",
     "shell.execute_reply": "2025-02-07T06:48:37.706737Z",
     "shell.execute_reply.started": "2025-02-07T06:48:37.695665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "prompts = [\"xlp\", \"xcot\", \"bnap\"]\n",
    "model_name = \"Llama_3.3_70B\"\n",
    "\n",
    "rate_limit = 6.0  # queries per minute\n",
    "sleep_interval = 60.0 / rate_limit  # 10 seconds per query\n",
    "\n",
    "def process_prompts_for_task(task_id, data):\n",
    "    \"\"\"\n",
    "    Process a given task with the provided dataset and for each prompt type.\n",
    "    \"\"\"\n",
    "    for prompt in prompts:\n",
    "        responses = []\n",
    "        chat_histories = []\n",
    "        num_instances = len(data)\n",
    "        print(f\"Processing Task {task_id} with prompt {prompt} for {num_instances} instances...\")\n",
    "\n",
    "        for i in range(num_instances):\n",
    "            start_time = time.time()\n",
    "            # Prepare variables for formatting prompt based on task type\n",
    "            if task_id in [1, 2, 4, 6]:\n",
    "                question = data.iloc[i]['Question']\n",
    "                formatted_prompt = prompt_map.get(f\"{prompt}_prompt_template_task1246\").format(question=question)\n",
    "            elif task_id == 3:\n",
    "                question = data.iloc[i]['Question']\n",
    "                option1 = data.iloc[i]['option1']\n",
    "                option2 = data.iloc[i]['option2']\n",
    "                formatted_prompt = prompt_map.get(f\"{prompt}_prompt_template_task{task_id}\").format(\n",
    "                    question=question, option1=option1, option2=option2\n",
    "                )\n",
    "            elif task_id == 5:\n",
    "                # For Task 5, data contains 'Premise' and 'Hypothesis'\n",
    "                premise = data.iloc[i]['Premise']\n",
    "                hypothesis = data.iloc[i]['Hypothesis']\n",
    "                formatted_prompt = prompt_map.get(f\"{prompt}_prompt_template_task{task_id}\").format(\n",
    "                    premise=premise, hypothesis=hypothesis\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Invalid task id: {task_id}\")\n",
    "                continue\n",
    "\n",
    "            # Start a new chat session and send the formatted prompt\n",
    "            model = load_model(prompting_type=prompt)\n",
    "            response = model.predict(formatted_prompt)\n",
    "            responses.append(response)\n",
    "\n",
    "            print(f\"\\033[91mTask {task_id} | Prompt {prompt} | Question No: {i}\\033[0m\")\n",
    "            \n",
    "            # Calculate elapsed time and sleep for the remaining time to complete 10 seconds.\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_sleep = sleep_interval - elapsed_time\n",
    "            if remaining_sleep > 0:\n",
    "                print(f\"Elapsed time: {elapsed_time:.2f}s | Remaining sleep: {remaining_sleep:.2f}s\")\n",
    "                time.sleep(remaining_sleep)\n",
    "                print(\"Sleeping for\", remaining_sleep, \"seconds...\")\n",
    "        \n",
    "        # Prepare the dataframe to save the responses and chat histories\n",
    "        data_with_solutions = copy.deepcopy(data)\n",
    "        data_with_solutions[\"Model Response\"] = responses\n",
    "        \n",
    "        # Save the responses to a CSV file\n",
    "        output_filename = f\"{model_name}_{prompt}_task{task_id}_random_{num_instances}_responses.csv\"\n",
    "        output_path = os.path.join(\"..\", \"Model Responses\",\"Llama_3.3_70B\", str(output_filename))\n",
    "        data_with_solutions.to_csv(output_path, index=False)\n",
    "        print(f\"Saved responses to {output_path}\")\n",
    "        \n",
    "        # Optionally, print a sample response (if exists)\n",
    "        print(data_with_solutions.iloc[10][\"Model Response\"])\n",
    "        \n",
    "        # print(\"Sleeping for 5 seconds before next prompt...\")\n",
    "        # time.sleep(5)\n",
    "    \n",
    "    print(\"Completed all prompts for Task\", task_id)\n",
    "    # print(\"Sleeping for 5 seconds before next task...\")\n",
    "    # time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T06:48:37.910111Z",
     "iopub.status.busy": "2025-02-07T06:48:37.909659Z",
     "iopub.status.idle": "2025-02-07T06:48:59.353903Z",
     "shell.execute_reply": "2025-02-07T06:48:59.352269Z",
     "shell.execute_reply.started": "2025-02-07T06:48:37.91008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_task(task_id):\n",
    "    \"\"\"\n",
    "    Load dataset for the task and process based on task type.\n",
    "    \"\"\"\n",
    "    dataset_key = f\"task{task_id}\"\n",
    "    data = load_dataset(dataset_key)\n",
    "    if isinstance(data, str):\n",
    "        print(f\"Error loading dataset for task: {task_id}\")\n",
    "        return\n",
    "\n",
    "    # For tasks 1, 2, 4, 6, 3, and 5 the processing is handled inside process_prompts_for_task.\n",
    "    process_prompts_for_task(task_id, data)\n",
    "\n",
    "def main():\n",
    "    for task in [6, 5, 4, 3, 2, 1]:\n",
    "        print(f\"Starting processing for Task {task}\")\n",
    "        process_task(task)\n",
    "    print(\"All tasks processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4922963,
     "sourceId": 9292863,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6604216,
     "sourceId": 10700644,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
